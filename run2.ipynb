{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8967,"status":"ok","timestamp":1650278235978,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"0caQOEuIMF9J"},"outputs":[],"source":["\"\"\"\n","    Returns a cleaned X and y dataset to be used for classification.\n","\"\"\"\n","import os\n","import cv2\n","import sys\n","import numpy as np\n","from PIL import Image\n","import tensorflow as tf\n","from scipy import misc\n","from matplotlib.image import imread\n","import matplotlib.pyplot as plt\n","import keras\n","\n","import joblib\n","from sklearn.model_selection import train_test_split, cross_validate\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from tensorflow.keras.utils import plot_model\n","\n","\n","from random import randint\n","import cv2\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from keras import optimizers\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4209,"status":"ok","timestamp":1650278240170,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"M8Of_yTlMJ1c","outputId":"c969d1dc-998f-479f-aec7-1a4ffab42044"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650278240170,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"XNWn14I8MqHW","outputId":"c7ef4653-13e9-4d57-c125-d783f98f9991"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":3}],"source":["# Uncomment the following to check whether you have access to a GPU in Google Colab\n","# See further instructione below.\n","import tensorflow as tf\n","tf.config.experimental.list_physical_devices('GPU') "]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650278240478,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"oOCuSnwMMu-j","outputId":"c7c9c2bd-e054-4d43-df81-7f76d68865f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looks good. You may continue :)\n"]}],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from packaging import version\n","import sklearn\n","import tensorflow as tf\n","tensorflow_version = tf.__version__\n","if version.parse(tensorflow_version) < version.parse(\"2.2.0\"):\n","    print(\"Tensorflow is outdated. This is version {}. Please update to 2.2 or later (e.g. 2.4)\".format(tensorflow_version))\n","elif version.parse(tensorflow_version) < version.parse(\"2.4.0\"):\n","    print(\"Tensorflow version is <2.4. This will likely work but we recommend updating to 2.4\".format(tensorflow_version))\n","else:\n","    print(\"Looks good. You may continue :)\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1650278240478,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"oSBSepn-MMMu"},"outputs":[],"source":["PATH=r\"C:\\Users\\Jeroen Gijsbers\\OneDrive - TU Eindhoven\\Uni\\Master\\Jaar 1\\Kwartiel 3\\2AMV10 - Visual Analytics\\Project\\2AMV10-group-16\"\n","PATH_m = r\"/content/drive/MyDrive/-Univ/2AMV10 Visual Analytics/2AMV10-group-16\"\n","IMG_SIZE  = 160\n","IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n","IMAGE_CHANNEL = 3\n","stop_training = False"]},{"cell_type":"markdown","metadata":{"id":"4jGetA9cMUBz"},"source":["Helper functions"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650278240479,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"},"user_tz":-120},"id":"ui5x4nC8MRWt"},"outputs":[],"source":["def process_img(sample):\n","  try:\n","    # print(sample.shape)\n","    sample = np.array(sample)\n","    sample = sample / 255.\n","    sample = cv2.resize(sample, (IMG_SIZE, IMG_SIZE))\n","    return sample.reshape(IMG_SIZE, IMG_SIZE, IMAGE_CHANNEL)\n","  except ValueError or AttributeError:\n","    print(sample, \"returned an error\")\n","\n","\n","def read_data():\n","    \"\"\"\n","        Reads the images and returns them as arrays X and y\n","\n","        returns: X      array containing np-array-representation of images\n","                 y      array containing corresponding labels\n","    \"\"\"\n","    #print(os.curdir)\n","    #os.chdir('../data/trainingData/TrainingImages')\n","    X = []\n","    y = []\n","    labels = []\n","    sys.path.insert(0, PATH_m)\n","\n","    path = \"/content/drive/MyDrive/-Univ/2AMV10/2AMV10-group-16/data/trainingData/TrainingImages\"\n","    i = 0\n","    limit = 100000\n","    for label in [name for name in os.listdir(path) if os.path.isdir(f\"{path}/{name}\")]:\n","        print(label)\n","        labels.append(label)\n","        for image in [name for name in os.listdir(f\"{path}/{label}\")]:\n","            if image.endswith('.db'):\n","              pass\n","            else:\n","              # print(f\"{path}/{label}/{image}\")\n","              X.append(process_img(cv2.imread(f\"{path}/{label}/{image}\")))\n","              # print(label)\n","              y.append(label)\n","              i+= 1\n","            if i == limit:\n","                break\n","        if i == limit:\n","            break\n","    return np.array(X), y, labels#.reshape(50, 256*256), y, labels\n","\n","\n","# Helper function for user feedback\n","def shout(text, verbose=1):\n","    \"\"\" Prints text in red. Just for fun.\n","    \"\"\"\n","    if verbose>0:\n","        print('\\033[91m'+text+'\\x1b[0m')\n","\n","def show_images(grayscale=False, display_test=False):\n","        \"\"\"\n","            Shows 10 random images from the dataset\n","        \"\"\"\n","        def plot_images(X, y, grayscale=False, display_test=False):\n","            fig, axes = plt.subplots(1, len(X), figsize=(15,30))\n","            for n in range(len(X)):\n","                if grayscale:\n","                    axes[n].imshow(X[n], cmap='gray')\n","                else:\n","                    axes[n].imshow(X[n])\n","                # print(f\"1: {y[n]}\")\n","                # print(f\"2: {np.argmax(y[n])} \")\n","                # print(f\"3: {self.labels[:5]}\")\n","                # print(f\"4: {self.labels[np.argmin(y[n])]}\")\n","                axes[n].set_xlabel(y[n])\n","                axes[n].set_xticks(()), axes[n].set_yticks(())\n","            plt.show()\n","\n","        images = [randint(0,len(X_train)-1) for i in range(5)]\n","        # print(images)\n","        X_random = [X_train[i] for i in images]\n","        y_random = [y_train[i] for i in images]\n","        plot_images(X_random, y_random)\n","\n","        if display_test:\n","            images = [randint(0,len(X_test)-1) for i in range(5)]\n","            X_random = [X_test[i] for i in images]\n","            y_random = [y_test[i] for i in images]\n","            plot_images(X_random, y_random)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb7awqi-McOB","outputId":"9ee9220b-32d4-4a34-cc22-554154f32bdf"},"outputs":[{"output_type":"stream","name":"stdout","text":["legoBracelet\n","metalKey\n","paperPlate\n","spiderRing\n","vancouverCards\n"]}],"source":["X, y, labels = read_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VPYDEgg9Nia"},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmNryVbA8IXV"},"outputs":[],"source":["len(np.unique(y))"]},{"cell_type":"code","source":["len(labels)"],"metadata":{"id":"ZdUKGRTdLLN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"undj8W8l91v1"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r95ge3Lf9S19"},"outputs":[],"source":["plt.imshow(X[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdv_JO54S1yD"},"outputs":[],"source":["labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jwc4rWpkS3RJ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTNWGB2qXLHg"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlTPDKazYGWb"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFCxnp9-NLVF"},"outputs":[],"source":["# Don't change the name of these variables\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n","\n","X_train = np.repeat(X_train, 10, axis=0)\n","y_train = np.repeat(y_train, 10, axis=0)\n","print(X_train.shape, y_train.shape)\n","\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMFDxoDcqJS3"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Qvq5Tg9oxpj"},"outputs":[],"source":["# labels = pd.unique(y_train)\n","# len(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Zwxm8qloK3z"},"outputs":[],"source":["from sklearn import preprocessing\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","\n","le.fit(y_val)\n","y_val = le.transform(y_val)\n","\n","le.fit(y_test)\n","y_test = le.transform(y_test)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4_6xDtZunge"},"outputs":[],"source":["y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4AxDJfhtb_a"},"outputs":[],"source":["le.fit(y)\n","y = le.transform(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4k6kszOo4eg"},"outputs":[],"source":["label_dict = dict(zip(pd.unique(y_train), labels))\n","classes = label_dict\n","classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qgyYZdyrzWn"},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","y_val = to_categorical(y_val)\n","y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fzB8U_37X0i4"},"outputs":[],"source":["y_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iI0njiP8X1hF"},"outputs":[],"source":["y_val.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBoaU8w1X2VK"},"outputs":[],"source":["y_test.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COyHYY_1OU0l"},"outputs":[],"source":["plt.imshow(X_test[3])\n","print(classes[np.argmax(y_test[3])])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icC2M630WINu"},"outputs":[],"source":["evaluation_split = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4apvo6uUOfRt"},"outputs":[],"source":["import os\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.models import load_model # for use with tensorflow\n","from tensorflow.keras.models import model_from_json\n","import pydot\n","from tensorflow.keras.utils import plot_model\n","from IPython.display import Image\n","import inspect\n","from IPython.core import page\n","page.page = print\n","\n","# Helper function for user feedback\n","def shout(text, verbose=1):\n","    \"\"\" Prints text in red. Just for fun.\n","    \"\"\"\n","    if verbose>0:\n","        print('\\033[91m'+text+'\\x1b[0m')\n","        \n","# Helper function to extract min/max from the learning curves\n","def minMax(x):\n","    return pd.Series(index=['min','max'],data=[x.min(),x.max()])\n","\n","# Helper function to format answers\n","def print_answer(ans):\n","    output = \"\"\n","    for line in ans.splitlines()[0:]:\n","        output += line.strip() + \" \"\n","    output += \"(length: \"+str(len(output))+\")\\n\"\n","    print(output)\n","    \n","\n","def load_model_from_file(base_dir, name, extension='.h5'):\n","    \"\"\" Loads a model from a file. The returned model must have a 'fit' and 'summary'\n","    function following the Keras API. Don't change if you use TensorFlow. Otherwise,\n","    adapt as needed. \n","    Keyword arguments:\n","    base_dir -- Directory where the models are stored\n","    name -- Name of the model, e.g. 'question_1_1'\n","    extension -- the file extension\n","    \"\"\"\n","    try:\n","        # if a json description is available, load config and then weights\n","        if os.path.isfile(os.path.join(base_dir, name+'.json')):\n","            json_file = open(os.path.join(base_dir, name+'.json'), 'r')\n","            loaded_model_json = json_file.read()\n","            json_file.close()\n","            model = model_from_json(loaded_model_json)\n","            model.load_weights(os.path.join(base_dir, name+extension))\n","        # else just load the entire model from hdf5 file\n","        else:\n","            model = load_model(os.path.join(base_dir, name+extension))\n","    except OSError:\n","        shout(\"Saved model could not be found. Was it trained and stored correctly? Is the base_dir correct?\")\n","        return False\n","    return model\n","\n","def save_model_to_file(model, base_dir, name, extension='.h5'):\n","    \"\"\" Saves a model to file. Don't change if you use TensorFlow. Otherwise,\n","    adapt as needed.\n","    Keyword arguments:\n","    model -- the model to be saved\n","    base_dir -- Directory where the models should be stored\n","    name -- Name of the model, e.g. 'question_1_1'\n","    extension -- the file extension\n","    \"\"\"\n","    path = os.path.join(base_dir, name+extension)\n","    model.save(path)\n","    size = os.path.getsize(path)\n","    # If model > 100MB, store the weights and architecture only.\n","    if size > 100*1024*1024:\n","        print(\"Model larger than 100MB, storing weights only.\")\n","        model.save_weights(path)\n","        model_json = model.to_json()\n","        with open(os.path.join(base_dir, name+\".json\"), \"w\") as json_file:\n","            json_file.write(model_json)\n","\n","# Evaluation harness\n","def run_evaluation(name, model_builder, data, base_dir, train=True, \n","                   generator=False, epochs=3, batch_size=32, steps_per_epoch=60, \n","                   verbose=1, print_model=True, **kwargs):\n","    \"\"\" Trains and evaluates the given model on the predefined train and test splits,\n","    stores the trained model and learning curves. Also prints out a summary of the \n","    model and plots the learning curves.\n","    Keyword arguments:\n","    name -- the name of the model to be stored, e.g. 'question_1_1.h5'\n","    model_builder -- function that returns an (untrained) model. The model must \n","                     have a 'fit' function that follows the Keras API. It can wrap\n","                     a non-Keras model as long as the 'fit' function takes the \n","                     same attributes and returns the learning curves (history).\n","                     It also must have a 'summary' function that prints out a \n","                     model summary, and a 'save' function that saves the model \n","                     to disk. \n","    data -- data split for evaluation. A tuple of either:\n","            * Numpy arrays (X_train, X_val, y_train, y_val)\n","            * A data generator and validation data (generator, X_val, y_val)\n","    base_dir -- the directory to save or read models to/from\n","    train -- whether or not the data should be trained. If False, the trained model\n","             will be loaded from disk.\n","    generator -- whether the data is given as a generator or not. Set batch size to None when using a generator.\n","    epochs -- the number of epochs to train for\n","    batch_size -- the batch size to train with. Set batch size to None when using a generator.\n","    steps_per_epoch -- steps per epoch, in case a generator is used (ignored otherwise)\n","    verbose -- verbosity level, 0: silent, 1: minimal,...\n","    print_model -- whether or not to print the model\n","    kwargs -- keyword arguments that should be passed to model_builder.\n","              Not required, but may help you to adjust its behavior\n","    \"\"\"\n","    model = model_builder(**kwargs)\n","    if not model:\n","        shout(\"No model is returned by the model_builder\")\n","        return\n","    if not hasattr(model, 'fit'):\n","        shout(\"Model is not built correctly\")\n","        return\n","    learning_curves = {}\n","\n","    if train and not stop_training: # Train anew\n","        shout(\"Training the model\", verbose)\n","        if generator:\n","            generator, X_val, y_val = data\n","            history = model.fit(generator, epochs=epochs, batch_size=batch_size,\n","                              steps_per_epoch=steps_per_epoch, verbose=1, \n","                              validation_data=(X_val, y_val), callbacks=[callback])\n","            learning_curves = history.history\n","        else:\n","            X_train, X_val, y_train, y_val = data\n","            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n","                              verbose=1, validation_data=(X_val, y_val))\n","            learning_curves = history.history\n","        shout(\"Saving to file\", verbose)\n","        save_model_to_file(model, base_dir, name)\n","        with open(os.path.join(base_dir, name+'.p'), 'wb') as file_pi:\n","            pickle.dump(learning_curves, file_pi)\n","        shout(\"Model stored in \"+base_dir, verbose)\n","    else: # Load from file\n","        model = load_model_from_file(base_dir, name)\n","        if not model:\n","            shout(\"Model not found\")\n","            return\n","        learning_curves = None\n","        try:\n","            learning_curves = pickle.load(open(os.path.join(base_dir, name+'.p'), \"rb\"))\n","        except FileNotFoundError:\n","            shout(\"Learning curves not found\")\n","            return\n","    # Report\n","    lc = pd.DataFrame(learning_curves)\n","    print(\"Max val score: {:.2f}%\".format(lc.iloc[:,3].max()*100))\n","    lc.plot(lw=2,style=['b:','r:','b-','r-']);\n","    plt.xlabel('epochs');\n","    plt.show()\n","    \n","    if print_model:\n","        print(model.summary())\n","    plot_model(model, to_file=os.path.join(base_dir,name+'.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VWLkkbThxdKA"},"outputs":[],"source":["print(y_train.shape[1])\n","print(X_train.shape, y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBNtXvGRuAQf"},"outputs":[],"source":["print(y_val.shape[1])\n","print(X_val.shape, y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I73-5W7A_jUv"},"outputs":[],"source":["print(y_test.shape[1])\n","print(X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yAO2LJMLzU3t"},"outputs":[],"source":["y = to_categorical(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWW2yUClzZqQ"},"outputs":[],"source":["y_val[5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYeNi-K75WS9"},"outputs":[],"source":["def augment_data():\n","    \"\"\" Augments the data and returns a generator and the validation data and labels\n","    \"\"\"\n","    # Boilerplate code. You can change this however you like.\n","    generator = ImageDataGenerator(\n","                                   height_shift_range=0.1,\n","                                   horizontal_flip=True,\n","                                   vertical_flip=True,\n","                                   fill_mode='nearest',\n","                                   rotation_range=360\n","                                   ).flow(X_train, y_train)\n","    return generator, X_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5zsyfF5OurV"},"outputs":[],"source":["# Toy usage example\n","from tensorflow.keras import models\n","from tensorflow.keras import layers \n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def build_toy_model():\n","    model = models.Sequential()\n","    model.add(MobileNetV2(input_shape=(160,160,3),\n","                        alpha=1.0,\n","                        include_top=False,\n","                        weights=\"imagenet\",\n","                        input_tensor=None,\n","                        pooling=None\n","                       ))\n","    # I tried unfreezing in the function below but it didnt work well\n","    for layer in model.layers[:1]:\n","      layer.trainable = False\n","\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dropout(0.3))\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(128, activation='relu'))\n","    model.add(layers.Dropout(0.4))\n","    model.add(layers.Dense(64, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(y_val.shape[1], activation='softmax'))\n","    model.compile(optimizer='rmsprop',\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    print(model.summary())\n","    return model\n","\n","\n","# First build and store\n"]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","callback = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"],"metadata":{"id":"vxMG2fBGDcdV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"URvp6rClYtHF"},"outputs":[],"source":["run_evaluation(\"toy_example\", build_toy_model, augment_data(), \n","               PATH_m, generator=True, epochs=100, steps_per_epoch=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O2Z5K0GbUyFA"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","\n","def evaluate_model(name):\n","    model = load_model_from_file(PATH_m, name, \".h5\")\n","    preds = np.argmax(model.predict(X_test), axis=1)\n","    y_test_c = np.argmax(y_test,axis=1)\n","    print(f\"The accuracy is: {accuracy_score(preds, y_test_c)}\")\n","    return accuracy_score(preds, y_test_c)\n","test_accuracy_2_1 = evaluate_model(\"toy_example\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfuLxldYddm6"},"outputs":[],"source":["model = load_model_from_file(PATH_m, 'toy_example', '.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRfpnCGVPb5K"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","classes = label_dict\n","\n","def plot_confusion_matrix():\n","    model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","    preds = np.argmax(model.predict(X_test), axis=1)\n","    y_test_c = np.argmax(y_test,axis=1)\n","\n","    cm = ConfusionMatrixDisplay(confusion_matrix(y_test_c, preds), display_labels=classes)\n","    cm.plot()\n","    plt.show()\n","plot_confusion_matrix()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tw_8ST7Ffhp4"},"outputs":[],"source":["def plot_misclassifications():\n","    model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","    y_pred = model.predict(X_test)\n","    misclassified_samples = np.nonzero(np.argmax(y_test, axis=1) != np.argmax(y_pred, axis=1))[0]\n","\n","    fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n","    print(misclassified_samples[:5])\n","    for nr, i in enumerate(misclassified_samples[:5]):\n","        axes[nr].imshow(X_test[i])\n","        print(np.argmax(y_pred[i]), np.argmax(y_test[i]))\n","        axes[nr].set_xlabel(\"Predicted: %s,\\n Actual : %s\" % (classes[np.argmax(y_pred[i])],classes[np.argmax(y_test[i])]))\n","        axes[nr].set_xticks(()), axes[nr].set_yticks(())\n","\n","    plt.show()\n","\n","\n","plot_misclassifications()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yflaYFAMyUes"},"outputs":[],"source":["plt.imshow(X_test[0]);\n","print(classes[np.argmax(y_test[0])])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIWqVmuksJcg"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgU93z3C-cCM"},"outputs":[],"source":["import json\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","def preds_to_json(test_set=X_test, plot=True):\n","  \"\"\"\n","  Function does not return anything but makes a new file, plots if you want it to\n","  \"\"\"\n","  \n","  X_test = test_set\n","  model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","  preds = model.predict(X_test)\n","\n","  a2 = np.argmax(preds, axis=1)\n","  a1 = np.argmax(y_test, axis=1)\n","  \n","  a_names = np.vectorize(classes.get)(a1)\n","  b_names = np.vectorize(classes.get)(a2)\n","\n","  data = confusion_matrix(a_names, b_names)\n","\n","  manual_dict = {}\n","  i = 0\n","  for col in data:\n","    colname = np.unique(a_names)[i]\n","    # print(colname)\n","\n","    temp_dict = {str(colname) : data[i].tolist()}\n","    # print(temp_dict)\n","    manual_dict.update(temp_dict)\n","    i+=1\n","\n","  with open('cm.json', 'w') as file:\n","      json.dump(manual_dict, file)\n","\n","  if plot:\n","    cm = ConfusionMatrixDisplay(confusion_matrix(a_names, b_names), display_labels=np.unique(a_names))\n","    cm.plot()\n","    plt.xticks(rotation = 45)\n","    plt.show()\n","\n","preds_to_json()"]},{"cell_type":"code","source":[""],"metadata":{"id":"2_XTFiqFwyew"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GXBtl37-pbG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVamTq97-gjB"},"outputs":[],"source":[""]},{"cell_type":"code","source":["print(PATH_m)"],"metadata":{"id":"zGb-DgcSyUKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time \n","def read_test():\n","\n","  \"\"\"\n","        Reads the images and returns them as arrays X and y\n","\n","        returns: X_t      array containing np-array-representation of images\n","                 y      array containing corresponding labels\n","  \"\"\"\n","\n","  #print(os.curdir)\n","  #os.chdir('../data/trainingData/TrainingImages')\n","  X_t = []\n","  captions = []\n","  person_order = []\n","  person_n = []\n","  sys.path.insert(0, PATH_m)\n","  t_0 = time.perf_counter()\n","  path = \"/content/drive/MyDrive/-Univ/2AMV10/2AMV10-group-16/data\"\n","  i = 0\n","  limit = 100000\n","  for label in [name for name in os.listdir(path) if os.path.isdir(f\"{path}/{name}\")]:\n","      print(label)\n","      if not 'Person' in label:\n","        break\n","      person_order.append(label)\n","      t_1 = time.perf_counter()\n","      for image in [name for name in os.listdir(f\"{path}/{label}\")]:\n","          if image.endswith('.db'):\n","            break\n","          if image.endswith('.txt'):\n","            break # skip captions for now\n","            # TODO get text out of file instead of filename\n","            # with open(f\"{path}/{label}/{image}\", \"r\") as f:\n","            #   print(image)\n","            #   print(f.readlines())\n","            #   captions.append(f.readlines())\n","          #X.append(cv2.imread(f\"{path}/{label}/{image}\"))\n","          else:\n","            person_n.append(label)\n","            X_t.append(process_img(cv2.imread(f\"{path}/{label}/{image}\")))\n","            t_2 = time.perf_counter()\n","            print(f\"Doing image {image} took {t_2-t_1}\")\n","            # print(label)\n","            i+= 1\n","          if i == limit:\n","              break\n","      if i == limit:\n","          break\n","      t_4 = time.perf_counter()\n","      print(f\"Doing person : {label} took {t_4 - t_1}\")\n","  t_3 = time.perf_counter()\n","  print(f\"Done in {t_3-t_0}\")  \n","  return np.array(X_t), captions, person_n#.reshape(50, 256*256), y, labels\n","\n","\n","X_t, captions, person_n = read_test()"],"metadata":{"id":"e4ttFuKc2Auz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_t[0]"],"metadata":{"id":"XPHlMWb04A9e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train[0]"],"metadata":{"id":"WXnC0U0C5AfS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hRzcwWPX3BTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_t[1] = tf.reshape(X_t[1], (160, 160, 3))"],"metadata":{"id":"MTm9E_aDoJmS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["yt_pred = model.predict(X_t)"],"metadata":{"id":"NldTEaFR9CRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.argmax(yt_pred[0])"],"metadata":{"id":"gYAa_EjhG-pS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["person_n[0]"],"metadata":{"id":"jQL3j6gzI2V8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(X_t[0])"],"metadata":{"id":"Bp6UwfUjmd7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","def test_to_json(test_set=X_t, plot=True):\n","  \"\"\"\n","  Function does not return anything but makes a new file, plots if you want it to\n","  \"\"\"\n","  \n","  X_test = test_set\n","  model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","  preds = model.predict(X_test)\n","\n","  \n","  a_names = np.vectorize(classes.get)(np.argmax(preds, axis=1))\n","\n","\n","  manual_dict = {}\n","  i = 0\n","  for col in data:\n","    colname = np.unique(a_names)[i]\n","    # print(colname)\n","\n","    temp_dict = {str(colname) : data[i].tolist()}\n","    # print(temp_dict)\n","    manual_dict.update(temp_dict)\n","    i+=1\n","\n","  with open('predsxt.json', 'w') as file:\n","      json.dump(manual_dict, file)\n","\n","\n","preds_to_json(plot=False)"],"metadata":{"id":"yssPbg5S8bEZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","preds = model.predict(X_t)\n","\n","\n","a_names = np.vectorize(classes.get)(np.argmax(preds, axis=1))\n","a_names.shape\n","\n","a_numbers = np.argmax(preds, axis=1)\n","print(a_numbers)\n"],"metadata":{"id":"fEx9-Bpx93yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classes"],"metadata":{"id":"53UWiwKZFwhK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(person_n)"],"metadata":{"id":"4fkz4zbD-Kfu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from itertools import groupby\n","\n","alist = [i for i, j in groupby(person_n)]\n","\n","blist = a_names\n","\n","listofpreds = list(zip(person_n, a_numbers))"],"metadata":{"id":"gGnOzmNp98fX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = {}                              # use a normal dictionary for our output\n","for k, v in listofpreds:                    # the keys may be duplicates\n","    results.setdefault(k[6:], []).append(int(v))\n","results"],"metadata":{"id":"BxWvmPplAttO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('predsxtnumerical.json', 'w') as file:\n","      json.dump(results, file)\n","      \n","      "],"metadata":{"id":"dJqFpoVk-dTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a_numbers = [int(x) for x in a_numbers]\n","a_numbers"],"metadata":{"id":"bKYBuiixi0j5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a_numbers = list(a_numbers)"],"metadata":{"id":"9G_JRxlnixjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["listofpreds = list(zip(person_n, a_numbers))"],"metadata":{"id":"NSzW1cg_A5AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"RjFEV5rrcqFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('predsxttuples.json', 'w') as file:\n","      json.dump(listofpreds, file)"],"metadata":{"id":"MVdR9lKhiVYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lvGFlmOSiaSO"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"run2.ipynb","provenance":[],"authorship_tag":"ABX9TyNQdgxZ8Vdyr2uBsTAH4Fyy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}