{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN7m4+xzmfo1dJN5IaCIJAq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"0caQOEuIMF9J","executionInfo":{"status":"ok","timestamp":1648999099534,"user_tz":-120,"elapsed":4647,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}}},"outputs":[],"source":["\"\"\"\n","    Returns a cleaned X and y dataset to be used for classification.\n","\"\"\"\n","import os\n","import cv2\n","import sys\n","import numpy as np\n","from PIL import Image\n","import tensorflow as tf\n","from scipy import misc\n","from matplotlib.image import imread\n","import matplotlib.pyplot as plt\n","import keras\n","\n","import joblib\n","from sklearn.model_selection import train_test_split, cross_validate\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from tensorflow.keras.utils import plot_model\n","\n","\n","from random import randint\n","import cv2\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from tensorflow import keras\n","from keras import optimizers\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M8Of_yTlMJ1c","executionInfo":{"status":"ok","timestamp":1648999103147,"user_tz":-120,"elapsed":3616,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}},"outputId":"b97ea6cc-d6f7-4f39-c8fd-56e2db389300"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Uncomment the following to check whether you have access to a GPU in Google Colab\n","# See further instructione below.\n","import tensorflow as tf\n","tf.config.experimental.list_physical_devices('GPU') "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XNWn14I8MqHW","executionInfo":{"status":"ok","timestamp":1648999103148,"user_tz":-120,"elapsed":15,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}},"outputId":"57cf27e7-0054-41d9-fe50-20f5a3348e04"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from packaging import version\n","import sklearn\n","import tensorflow as tf\n","tensorflow_version = tf.__version__\n","if version.parse(tensorflow_version) < version.parse(\"2.2.0\"):\n","    print(\"Tensorflow is outdated. This is version {}. Please update to 2.2 or later (e.g. 2.4)\".format(tensorflow_version))\n","elif version.parse(tensorflow_version) < version.parse(\"2.4.0\"):\n","    print(\"Tensorflow version is <2.4. This will likely work but we recommend updating to 2.4\".format(tensorflow_version))\n","else:\n","    print(\"Looks good. You may continue :)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oOCuSnwMMu-j","executionInfo":{"status":"ok","timestamp":1648999103148,"user_tz":-120,"elapsed":7,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}},"outputId":"35835d89-ed26-4c54-c05e-3d175abebd04"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looks good. You may continue :)\n"]}]},{"cell_type":"code","source":["PATH=r\"C:\\Users\\Jeroen Gijsbers\\OneDrive - TU Eindhoven\\Uni\\Master\\Jaar 1\\Kwartiel 3\\2AMV10 - Visual Analytics\\Project\\2AMV10-group-16\"\n","PATH_m = r\"/content/drive/MyDrive/-Univ/2AMV10 Visual Analytics/2AMV10-group-16\"\n","IMG_SIZE  = 160\n","IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n","stop_training = False"],"metadata":{"id":"oSBSepn-MMMu","executionInfo":{"status":"ok","timestamp":1648999103148,"user_tz":-120,"elapsed":5,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Helper functions"],"metadata":{"id":"4jGetA9cMUBz"}},{"cell_type":"code","source":["def process_img(sample):\n","  try:\n","    # print(sample.shape)\n","    sample = tf.cast(sample, tf.float32)\n","    sample = sample / 255.\n","    sample = tf.image.resize(sample, [IMG_SIZE, IMG_SIZE])\n","    return sample\n","  except ValueError or AttributeError:\n","    print(sample, \"returned an error\")\n","\n","\n","def read_data():\n","    \"\"\"\n","        Reads the images and returns them as arrays X and y\n","\n","        returns: X      array containing np-array-representation of images\n","                 y      array containing corresponding labels\n","    \"\"\"\n","    #print(os.curdir)\n","    #os.chdir('../data/trainingData/TrainingImages')\n","    X = []\n","    y = []\n","    labels = []\n","    sys.path.insert(0, PATH_m)\n","\n","    path = \"/content/drive/MyDrive/-Univ/2AMV10 Visual Analytics/2AMV10-group-16/data/trainingData/TrainingImages\"\n","    i = 0\n","    limit = 100000\n","    for label in [name for name in os.listdir(path) if os.path.isdir(f\"{path}/{name}\")]:\n","        print(label)\n","        labels.append(label)\n","        for image in [name for name in os.listdir(f\"{path}/{label}\")]:\n","            if image.endswith('.db'):\n","              break\n","            #X.append(cv2.imread(f\"{path}/{label}/{image}\"))\n","            try:\n","                # print(f\"{path}/{label}/{image}\")\n","                X.append(process_img(cv2.imread(f\"{path}/{label}/{image}\")))\n","                # print(label)\n","                y.append(label)\n","                i+= 1\n","            except:\n","                print(\"something went wrong, probably .db file\")\n","            if i == limit:\n","                break\n","        if i == limit:\n","            break\n","    return np.array(X), y, labels#.reshape(50, 256*256), y, labels\n","\n","\n","# Helper function for user feedback\n","def shout(text, verbose=1):\n","    \"\"\" Prints text in red. Just for fun.\n","    \"\"\"\n","    if verbose>0:\n","        print('\\033[91m'+text+'\\x1b[0m')\n","\n","def show_images(grayscale=False, display_test=False):\n","        \"\"\"\n","            Shows 10 random images from the dataset\n","        \"\"\"\n","        def plot_images(X, y, grayscale=False, display_test=False):\n","            fig, axes = plt.subplots(1, len(X), figsize=(15,30))\n","            for n in range(len(X)):\n","                if grayscale:\n","                    axes[n].imshow(X[n], cmap='gray')\n","                else:\n","                    axes[n].imshow(X[n])\n","                # print(f\"1: {y[n]}\")\n","                # print(f\"2: {np.argmax(y[n])} \")\n","                # print(f\"3: {self.labels[:5]}\")\n","                # print(f\"4: {self.labels[np.argmin(y[n])]}\")\n","                axes[n].set_xlabel(y[n])\n","                axes[n].set_xticks(()), axes[n].set_yticks(())\n","            plt.show()\n","\n","        images = [randint(0,len(X_train)-1) for i in range(5)]\n","        # print(images)\n","        X_random = [X_train[i] for i in images]\n","        y_random = [y_train[i] for i in images]\n","        plot_images(X_random, y_random)\n","\n","        if display_test:\n","            images = [randint(0,len(X_test)-1) for i in range(5)]\n","            X_random = [X_test[i] for i in images]\n","            y_random = [y_test[i] for i in images]\n","            plot_images(X_random, y_random)\n"],"metadata":{"id":"ui5x4nC8MRWt","executionInfo":{"status":"ok","timestamp":1648999103415,"user_tz":-120,"elapsed":272,"user":{"displayName":"matthijs keep","userId":"12327812061455129548"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X, y, labels = read_data()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb7awqi-McOB","outputId":"a5b076f1-ce23-4b0c-b85a-24fc4235974b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["legoBracelet\n","metalKey\n","paperPlate\n","spiderRing\n","vancouverCards\n","voiceRecorder\n","silverStraw\n","redBow\n","turtle\n","pumpkinNotes\n","rainbowPens\n","trophy\n","stickerBox\n","rubiksCube\n","noisemaker\n","partyFavor\n","miniCards\n","pinkEraser\n","yellowBag\n","redWhistle\n","plaidPencil\n","yellowBalloon\n","redDart\n"]}]},{"cell_type":"code","source":["X.shape"],"metadata":{"id":"4VPYDEgg9Nia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = np.repeat(X, 50, axis=0)\n","y = np.repeat(y, 50, axis=0)\n","print(X.shape, y.shape)"],"metadata":{"id":"hmNryVbA8IXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"undj8W8l91v1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(X[1])"],"metadata":{"id":"r95ge3Lf9S19"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels"],"metadata":{"id":"cdv_JO54S1yD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Jwc4rWpkS3RJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"CTNWGB2qXLHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MlTPDKazYGWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Don't change the name of these variables\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n","\n"],"metadata":{"id":"fFCxnp9-NLVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train"],"metadata":{"id":"pMFDxoDcqJS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = pd.unique(y_train)\n","labels"],"metadata":{"id":"0Qvq5Tg9oxpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(y_train)\n","y_train = le.transform(y_train)\n","\n","le.fit(y_val)\n","y_val = le.transform(y_val)\n","\n","le.fit(y_test)\n","y_test = le.transform(y_test)\n","\n","\n","\n"],"metadata":{"id":"-Zwxm8qloK3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_val"],"metadata":{"id":"D4_6xDtZunge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["le.fit(y)\n","y = le.transform(y)"],"metadata":{"id":"_4AxDJfhtb_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = dict(zip(pd.unique(y_train), labels))\n","classes = label_dict\n","classes"],"metadata":{"id":"x4k6kszOo4eg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","y_val = to_categorical(y_val)\n","y_val"],"metadata":{"id":"-qgyYZdyrzWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train.shape"],"metadata":{"id":"fzB8U_37X0i4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_val.shape"],"metadata":{"id":"iI0njiP8X1hF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test.shape\n"],"metadata":{"id":"JBoaU8w1X2VK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(X_test[3])\n","print(classes[np.argmax(y_test[3])])"],"metadata":{"id":"COyHYY_1OU0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation_split = np.array(X_train), np.array(X_val), np.array(y_train), np.array(y_val)"],"metadata":{"id":"icC2M630WINu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from tensorflow.keras.models import load_model # for use with tensorflow\n","from tensorflow.keras.models import model_from_json\n","import pydot\n","from tensorflow.keras.utils import plot_model\n","from IPython.display import Image\n","import inspect\n","from IPython.core import page\n","page.page = print\n","\n","# Helper function for user feedback\n","def shout(text, verbose=1):\n","    \"\"\" Prints text in red. Just for fun.\n","    \"\"\"\n","    if verbose>0:\n","        print('\\033[91m'+text+'\\x1b[0m')\n","        \n","# Helper function to extract min/max from the learning curves\n","def minMax(x):\n","    return pd.Series(index=['min','max'],data=[x.min(),x.max()])\n","\n","# Helper function to format answers\n","def print_answer(ans):\n","    output = \"\"\n","    for line in ans.splitlines()[0:]:\n","        output += line.strip() + \" \"\n","    output += \"(length: \"+str(len(output))+\")\\n\"\n","    print(output)\n","    \n","\n","def load_model_from_file(base_dir, name, extension='.h5'):\n","    \"\"\" Loads a model from a file. The returned model must have a 'fit' and 'summary'\n","    function following the Keras API. Don't change if you use TensorFlow. Otherwise,\n","    adapt as needed. \n","    Keyword arguments:\n","    base_dir -- Directory where the models are stored\n","    name -- Name of the model, e.g. 'question_1_1'\n","    extension -- the file extension\n","    \"\"\"\n","    try:\n","        # if a json description is available, load config and then weights\n","        if os.path.isfile(os.path.join(base_dir, name+'.json')):\n","            json_file = open(os.path.join(base_dir, name+'.json'), 'r')\n","            loaded_model_json = json_file.read()\n","            json_file.close()\n","            model = model_from_json(loaded_model_json)\n","            model.load_weights(os.path.join(base_dir, name+extension))\n","        # else just load the entire model from hdf5 file\n","        else:\n","            model = load_model(os.path.join(base_dir, name+extension))\n","    except OSError:\n","        shout(\"Saved model could not be found. Was it trained and stored correctly? Is the base_dir correct?\")\n","        return False\n","    return model\n","\n","def save_model_to_file(model, base_dir, name, extension='.h5'):\n","    \"\"\" Saves a model to file. Don't change if you use TensorFlow. Otherwise,\n","    adapt as needed.\n","    Keyword arguments:\n","    model -- the model to be saved\n","    base_dir -- Directory where the models should be stored\n","    name -- Name of the model, e.g. 'question_1_1'\n","    extension -- the file extension\n","    \"\"\"\n","    path = os.path.join(base_dir, name+extension)\n","    model.save(path)\n","    size = os.path.getsize(path)\n","    # If model > 100MB, store the weights and architecture only.\n","    if size > 100*1024*1024:\n","        print(\"Model larger than 100MB, storing weights only.\")\n","        model.save_weights(path)\n","        model_json = model.to_json()\n","        with open(os.path.join(base_dir, name+\".json\"), \"w\") as json_file:\n","            json_file.write(model_json)\n","\n","# Evaluation harness\n","def run_evaluation(name, model_builder, data, base_dir, train=True, \n","                   generator=False, epochs=3, batch_size=32, steps_per_epoch=60, \n","                   verbose=1, print_model=True, **kwargs):\n","    \"\"\" Trains and evaluates the given model on the predefined train and test splits,\n","    stores the trained model and learning curves. Also prints out a summary of the \n","    model and plots the learning curves.\n","    Keyword arguments:\n","    name -- the name of the model to be stored, e.g. 'question_1_1.h5'\n","    model_builder -- function that returns an (untrained) model. The model must \n","                     have a 'fit' function that follows the Keras API. It can wrap\n","                     a non-Keras model as long as the 'fit' function takes the \n","                     same attributes and returns the learning curves (history).\n","                     It also must have a 'summary' function that prints out a \n","                     model summary, and a 'save' function that saves the model \n","                     to disk. \n","    data -- data split for evaluation. A tuple of either:\n","            * Numpy arrays (X_train, X_val, y_train, y_val)\n","            * A data generator and validation data (generator, X_val, y_val)\n","    base_dir -- the directory to save or read models to/from\n","    train -- whether or not the data should be trained. If False, the trained model\n","             will be loaded from disk.\n","    generator -- whether the data is given as a generator or not. Set batch size to None when using a generator.\n","    epochs -- the number of epochs to train for\n","    batch_size -- the batch size to train with. Set batch size to None when using a generator.\n","    steps_per_epoch -- steps per epoch, in case a generator is used (ignored otherwise)\n","    verbose -- verbosity level, 0: silent, 1: minimal,...\n","    print_model -- whether or not to print the model\n","    kwargs -- keyword arguments that should be passed to model_builder.\n","              Not required, but may help you to adjust its behavior\n","    \"\"\"\n","    model = model_builder(**kwargs)\n","    if not model:\n","        shout(\"No model is returned by the model_builder\")\n","        return\n","    if not hasattr(model, 'fit'):\n","        shout(\"Model is not built correctly\")\n","        return\n","    learning_curves = {}\n","\n","    if train and not stop_training: # Train anew\n","        shout(\"Training the model\", verbose)\n","        if generator:\n","            generator, X_val, y_val = data\n","            history = model.fit(generator, epochs=epochs, batch_size=batch_size,\n","                              steps_per_epoch=steps_per_epoch, verbose=1, \n","                              validation_data=(X_val, y_val))\n","            learning_curves = history.history\n","        else:\n","            X_train, X_val, y_train, y_val = data\n","            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n","                              verbose=1, validation_data=(X_val, y_val))\n","            learning_curves = history.history\n","        shout(\"Saving to file\", verbose)\n","        save_model_to_file(model, base_dir, name)\n","        with open(os.path.join(base_dir, name+'.p'), 'wb') as file_pi:\n","            pickle.dump(learning_curves, file_pi)\n","        shout(\"Model stored in \"+base_dir, verbose)\n","    else: # Load from file\n","        model = load_model_from_file(base_dir, name)\n","        if not model:\n","            shout(\"Model not found\")\n","            return\n","        learning_curves = None\n","        try:\n","            learning_curves = pickle.load(open(os.path.join(base_dir, name+'.p'), \"rb\"))\n","        except FileNotFoundError:\n","            shout(\"Learning curves not found\")\n","            return\n","    # Report\n","    lc = pd.DataFrame(learning_curves)\n","    print(\"Max val score: {:.2f}%\".format(lc.iloc[:,3].max()*100))\n","    lc.plot(lw=2,style=['b:','r:','b-','r-']);\n","    plt.xlabel('epochs');\n","    plt.show()\n","    \n","    if print_model:\n","        print(model.summary())\n","    plot_model(model, to_file=os.path.join(base_dir,name+'.png'))"],"metadata":{"id":"4apvo6uUOfRt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_train.shape[1])\n","print(X_train.shape, y_train.shape)"],"metadata":{"id":"VWLkkbThxdKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_val.shape[1])\n","print(X_val.shape, y_val.shape)"],"metadata":{"id":"nBNtXvGRuAQf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_test.shape[1])\n","print(X_test.shape, X_test.shape)"],"metadata":{"id":"I73-5W7A_jUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = to_categorical(y)"],"metadata":{"id":"yAO2LJMLzU3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_val[5]"],"metadata":{"id":"MWW2yUClzZqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def augment_data():\n","    \"\"\" Augments the data and returns a generator and the validation data and labels\n","    \"\"\"\n","    # Boilerplate code. You can change this however you like.\n","    generator = ImageDataGenerator(\n","                                   height_shift_range=0.1,\n","                                   shear_range=0.1, \n","                                   horizontal_flip=True,\n","                                   vertical_flip=False,\n","                                   fill_mode='nearest',\n","                                   rotation_range=30\n","                                   ).flow(X_train, y_train)\n","    return generator, X_test, y_test"],"metadata":{"id":"VYeNi-K75WS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Toy usage example\n","from tensorflow.keras import models\n","from tensorflow.keras import layers \n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def build_toy_model():\n","    model = models.Sequential()\n","    model.add(MobileNetV2(input_shape=(160,160,3),\n","                        alpha=1.0,\n","                        include_top=False,\n","                        weights=\"imagenet\",\n","                        input_tensor=None,\n","                        pooling=None\n","                       ))\n","    # I tried unfreezing in the function below but it didnt work well\n","    for layer in model.layers:\n","      layer.trainable = False\n","\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dropout(0.3))\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(256, activation='relu'))\n","    model.add(layers.Dropout(0.4))\n","    model.add(layers.Dense(128, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(y_val.shape[1], activation='softmax'))\n","    model.compile(optimizer='rmsprop',\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    print(model.summary())\n","    return model\n","\n","\n","# First build and store\n"],"metadata":{"id":"a5zsyfF5OurV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_evaluation(\"toy_example\", build_toy_model, augment_data(), \n","               PATH_m, generator=True, epochs=25, batch_size=None, steps_per_epoch=30)"],"metadata":{"id":"URvp6rClYtHF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","\n","def evaluate_model(name):\n","    model = load_model_from_file(PATH_m, name, \".h5\")\n","    preds = np.argmax(model.predict(X_test), axis=1)\n","    y_test_c = np.argmax(y_test,axis=1)\n","    print(f\"The accuracy is: {accuracy_score(preds, y_test_c)}\")\n","    return accuracy_score(preds, y_test_c)\n","test_accuracy_2_1 = evaluate_model(\"toy_example\")\n"],"metadata":{"id":"O2Z5K0GbUyFA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"BfuLxldYddm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","classes = label_dict\n","\n","def plot_confusion_matrix():\n","    model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","    preds = np.argmax(model.predict(X_test), axis=1)\n","    y_test_c = np.argmax(y_test,axis=1)\n","\n","    cm = ConfusionMatrixDisplay(confusion_matrix(y_test_c, preds), display_labels=classes)\n","    cm.plot()\n","    plt.show()\n","plot_confusion_matrix()\n","\n","\n"],"metadata":{"id":"gRfpnCGVPb5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_misclassifications():\n","    model = load_model_from_file(PATH_m, \"toy_example\", \".h5\")\n","    y_pred = model.predict(X_test)\n","    misclassified_samples = np.nonzero(np.argmax(y_test, axis=1) != np.argmax(y_pred, axis=1))[0]\n","\n","    fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n","    print(misclassified_samples[:5])\n","    for nr, i in enumerate(misclassified_samples[:5]):\n","        axes[nr].imshow(X_test[i])\n","        print(np.argmax(y_pred[i]), np.argmax(y_test[i]))\n","        axes[nr].set_xlabel(\"Predicted: %s,\\n Actual : %s\" % (classes[np.argmax(y_pred[i])],classes[np.argmax(y_test[i])]))\n","        axes[nr].set_xticks(()), axes[nr].set_yticks(())\n","\n","    plt.show();\n","\n","\n","plot_misclassifications()"],"metadata":{"id":"Tw_8ST7Ffhp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.imshow(X_test[7]);\n","print(classes[np.argmax(y_test[7])])\n"],"metadata":{"id":"EanIWquogaXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7756h-2_iCt9"},"execution_count":null,"outputs":[]}]}